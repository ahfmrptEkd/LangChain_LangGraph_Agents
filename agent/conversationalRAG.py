from dotenv import load_dotenv
import bs4
import faiss

from langchain_openai import OpenAIEmbeddings

from langchain.chat_models import init_chat_model

from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.tools import tool
from langchain_core.messages import SystemMessage
from langchain_chroma import Chroma

from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

from langgraph.graph import MessagesState, StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode, tools_condition, create_react_agent

load_dotenv()


class ConversationalRAG:
    """
    A conversational RAG (Retrieval-Augmented Generation) implementation using LangChain.
    
    This class provides both Chain-based and Agent-based RAG capabilities
    with conversation memory and multiple vector store options.
    """
    
    def __init__(self):
        """
        Initialize the ConversationalRAG system.
        Sets up LLM, embeddings, and prepares for vector store configuration.
        """
        self.llm = init_chat_model("gpt-4o-mini", model_provider="openai")
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        self.vector_store = None
        self.workflow = None
        self.app = None
        self.agent_app = None
        self.memory = MemorySaver()
        
    def setup_vector_store(self, choice: str = "1"):
        """
        Set up the vector store based on user choice.
        
        Args:
            choice: Vector store choice ("1" for InMemory, "2" for Chroma, "3" for FAISS)
        """
        print("ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì„¤ì •í•˜ëŠ” ì¤‘...")
        
        if choice == "1":
            print("InMemory ë²¡í„° ìŠ¤í† ì–´ ì‚¬ìš©")
            self.vector_store = InMemoryVectorStore(self.embeddings)
        elif choice == "2":
            print("Chroma ë²¡í„° ìŠ¤í† ì–´ ì‚¬ìš©")
            self.vector_store = Chroma(
                collection_name="example_collection",
                embedding_function=self.embeddings,
                persist_directory="./chroma_langchain_db",
            )
        elif choice == "3":
            print("FAISS ë²¡í„° ìŠ¤í† ì–´ ì‚¬ìš©")
            faiss_embedding_dim = len(self.embeddings.embed_query("Hello, world!"))
            index = faiss.IndexFlatL2(faiss_embedding_dim)
            self.vector_store = FAISS(
                embedding_function=self.embeddings,
                index=index,
                docstore=InMemoryDocstore(),
                index_to_docstore_id={}
            )
        else:
            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. InMemoryë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.vector_store = InMemoryVectorStore(self.embeddings)
    
    def load_documents(self):
        """
        Load and process documents from web source.
        Uses Lilian Weng's agent blog post as the knowledge base.
        """
        print("ë¬¸ì„œë¥¼ ë¡œë”©í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ì¤‘...")
        
        # Load documents from web
        loader = WebBaseLoader(
            web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
            bs_kwargs=dict(
                parse_only=bs4.SoupStrainer(
                    class_=("post-content", "post-title", "post-header")
                )
            ),
        )
        docs = loader.load()
        
        # Split documents into chunks
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        all_splits = text_splitter.split_documents(docs)
        
        # Add documents to vector store
        self.vector_store.add_documents(documents=all_splits)
        print(f"ì´ {len(all_splits)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def create_retrieval_tool(self):
        """
        Create the retrieval tool for document search.
        
        Returns:
            tool: LangChain tool for document retrieval
        """
        @tool(response_format="content_and_artifact")
        def retrieve(query: str):
            """Retrieve information related to a query."""
            retrieved_docs = self.vector_store.similarity_search(query, k=2)
            serialized = "\n\n".join(
                (f"Source: {doc.metadata}\n" f"content: {doc.page_content}")
                for doc in retrieved_docs
            )
            return serialized, retrieved_docs
        
        return retrieve
    
    def setup_workflow(self):
        """
        Set up the Chain-based RAG workflow with query processing, retrieval, and generation steps.
        """
        retrieve_tool = self.create_retrieval_tool()
        
        def query_or_response(state: MessagesState):
            """
            Generate tool call for retrieval or respond directly.
            
            Args:
                state: Current conversation state
                
            Returns:
                Dictionary containing the response message
            """
            llm_with_tools = self.llm.bind_tools([retrieve_tool])
            response = llm_with_tools.invoke(state["messages"])
            return {"messages": [response]}
        
        def generate(state: MessagesState):
            """
            Generate answer using retrieved content.
            
            Args:
                state: Current conversation state containing tool messages
                
            Returns:
                Dictionary containing the generated response
            """
            # Get generated ToolMessages
            recent_tool_messages = []
            for message in reversed(state["messages"]):
                if message.type == "tool":
                    recent_tool_messages.append(message)
                else:
                    break
            
            tool_messages = recent_tool_messages[::-1]
            
            # Format into prompt
            docs_content = "\n\n".join(doc.content for doc in tool_messages)
            system_message_content = (
                f"""You are an assistant for question-answering tasks.
                Use the following pieces of retrieved context to answer the question.
                If you don't know the answer, just say that you don't know.
                Do not try to make up an answer.
                Use three sentences maximum and keep the answer concise.
                \n\n
                {docs_content}
                """
            )
            
            # Filter conversation messages
            conversation_messages = [
                message
                for message in state["messages"]
                if message.type in ("human", "system")
                or (message.type == "ai" and not message.tool_calls)
            ]
            
            prompt = [SystemMessage(content=system_message_content)] + conversation_messages
            response = self.llm.invoke(prompt)
            return {"messages": [response]}
        
        # Create workflow
        self.workflow = StateGraph(MessagesState)
        tools = ToolNode([retrieve_tool])
        
        # Add nodes
        self.workflow.add_node("query_or_response", query_or_response)
        self.workflow.add_node("tools", tools)
        self.workflow.add_node("generate", generate)
        
        # Add edges
        self.workflow.add_edge(START, "query_or_response")
        self.workflow.add_conditional_edges(
            "query_or_response",
            tools_condition,
            {
                END: END,
                "tools": "tools"
            }
        )
        self.workflow.add_edge("tools", "generate")
        self.workflow.add_edge("generate", END)
        
        # Compile workflow
        self.app = self.workflow.compile(checkpointer=self.memory)
    
    def setup_agent_workflow(self):
        """
        Set up the Agent-based RAG workflow.
        Agent can decide when to search and how many times to search.
        """
        retrieve_tool = self.create_retrieval_tool()
        
        # System message for the agent
        system_message = """You are a helpful assistant with access to a document retrieval tool.
        
        When a user asks a question:
        1. If the question is about general topics or common knowledge, you can answer directly without searching.
        2. If the question requires specific information from the documents (especially about AI agents, task decomposition, planning, memory, etc.), use the retrieve tool to find relevant information.
        3. You can use the retrieve tool multiple times if needed to get comprehensive information.
        4. After retrieving information, provide a clear and concise answer based on the retrieved content.
        5. If you can't find relevant information in the documents, say so honestly.
        
        Be conversational and helpful while being accurate with the information."""
        
        # Store system message for later use
        self.agent_system_message = system_message
        
        # Create agent using create_react_agent
        self.agent_app = create_react_agent(
            self.llm,
            tools=[retrieve_tool],
            checkpointer=self.memory
        )
    
    def run_basic_test(self):
        """
        Run basic Chain-based RAG functionality test without memory.
        """
        print("\n=== ê¸°ë³¸ Chain RAG í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ì—†ìŒ) ===")
        
        # Create app without memory for basic test
        basic_app = self.workflow.compile()
        
        test_query = "What is Task Decomposition?"
        print(f"\nì§ˆë¬¸: {test_query}")
        print("ë‹µë³€:")
        
        for step in basic_app.stream(
            {"messages": [{"role": "user", "content": test_query}]},
            stream_mode="values"
        ):
            if step["messages"][-1].type == "ai" and not step["messages"][-1].tool_calls:
                print(f"Bot: {step['messages'][-1].content}")
    
    def run_persistent_test(self):
        """
        Run Chain-based RAG test with conversation memory.
        """
        print("\n=== ì§€ì†ì  Chain RAG í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ìˆìŒ) ===")
        
        config = {"configurable": {"thread_id": "test_chain_rag_conversation"}}
        
        # First question
        query1 = "What is Task Decomposition?"
        print(f"\n1. ì²« ë²ˆì§¸ ì§ˆë¬¸: {query1}")
        print("ë‹µë³€:")
        
        for step in self.app.stream(
            {"messages": [{"role": "user", "content": query1}]},
            stream_mode="values",
            config=config,
        ):
            if step["messages"][-1].type == "ai" and not step["messages"][-1].tool_calls:
                print(f"Bot: {step['messages'][-1].content}")
        
        # Follow-up question
        query2 = "What are the main challenges with it?"
        print(f"\n2. í›„ì† ì§ˆë¬¸: {query2}")
        print("ë‹µë³€:")
        
        for step in self.app.stream(
            {"messages": [{"role": "user", "content": query2}]},
            stream_mode="values",
            config=config,
        ):
            if step["messages"][-1].type == "ai" and not step["messages"][-1].tool_calls:
                print(f"Bot: {step['messages'][-1].content}")
    
    def run_agent_test(self):
        """
        Run Agent-based RAG test.
        Agent decides when to search and how many times to search.
        """
        print("\n=== Agent ê¸°ë°˜ RAG í…ŒìŠ¤íŠ¸ ===")
        
        config = {"configurable": {"thread_id": "test_agent_conversation"}}
        
        # Test questions for agent
        test_questions = [
            "Hello, how are you?",  # General question - should not trigger search
            "What is Task Decomposition?",  # Document-specific - should trigger search
            "Can you tell me more about the challenges?",  # Follow-up - should use context
            "What's the weather like today?",  # Unrelated - should not search
        ]
        
        for i, question in enumerate(test_questions, 1):
            print(f"\n{i}. ì§ˆë¬¸: {question}")
            print("="*60)
            
            # For first question, include system message
            if i == 1:
                messages = [
                    {"role": "system", "content": self.agent_system_message},
                    {"role": "user", "content": question}
                ]
            else:
                messages = [{"role": "user", "content": question}]
            
            # Track agent steps
            step_count = 0
            tool_call_count = 0
            final_response = None
            
            print("ğŸ¤– Agent ì¶”ë¡  ê³¼ì •:")
            print("-"*40)
            
            # Stream agent response with detailed steps
            for chunk in self.agent_app.stream(
                {"messages": messages},
                config=config,
                stream_mode="updates"  # Changed to see each step
            ):
                step_count += 1
                
                # Process each node update
                for node_name, node_data in chunk.items():
                    if node_name == "__start__":
                        continue
                        
                    print(f"ğŸ“ Step {step_count}: {node_name}")
                    
                    if "messages" in node_data:
                        last_message = node_data["messages"][-1]
                        
                        # Check if this is a tool call
                        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                            tool_call_count += 1
                            tool_name = last_message.tool_calls[0]['name']
                            tool_args = last_message.tool_calls[0]['args']
                            print(f"  ğŸ”§ ë„êµ¬ í˜¸ì¶œ #{tool_call_count}: {tool_name}")
                            print(f"     ğŸ“‹ ê²€ìƒ‰ì–´: {tool_args.get('query', 'N/A')}")
                            
                        # Check if this is a tool result
                        elif hasattr(last_message, 'type') and last_message.type == "tool":
                            print("  ğŸ“„ ë„êµ¬ ê²°ê³¼: ê²€ìƒ‰ ì™„ë£Œ")
                            print(f"     ğŸ“Š ë¬¸ì„œ ë°œê²¬: {len(last_message.content.split('Source:')) - 1}ê°œ")
                            
                        # Check if this is final AI response
                        elif hasattr(last_message, 'content') and last_message.type == "ai" and not (hasattr(last_message, 'tool_calls') and last_message.tool_calls):
                            print("  ğŸ’­ ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
                            final_response = last_message
                            
                    print()
            
            # Summary
            print("-"*40)
            print("ğŸ“Š Agent í™œë™ ìš”ì•½:")
            print(f"   â€¢ ì´ ë‹¨ê³„: {step_count}")
            print(f"   â€¢ ë„êµ¬ í˜¸ì¶œ íšŸìˆ˜: {tool_call_count}")
            print(f"   â€¢ ìµœì¢… ë‹µë³€: {'ìƒì„±ë¨' if final_response else 'ì—†ìŒ'}")
            print("-"*40)
            
            # Print final response
            if final_response and hasattr(final_response, 'content'):
                print(f"ğŸ¯ ìµœì¢… ë‹µë³€:\n{final_response.content}")
            else:
                print("âŒ ì‘ë‹µ ì—†ìŒ")
            
            print("="*60)
    
    def interactive_chat(self):
        """
        Start an interactive Chain-based RAG chat session.
        Users can ask questions about the loaded documents.
        """
        print("\n=== ì¸í„°ë™í‹°ë¸Œ Chain RAG ì±„íŒ… ===")
        print("ë¡œë”©ëœ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”. 'quit'ì„ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
        print("(ì£¼ì œ: AI Agentì˜ Task Decomposition, Planning, Memory ë“±)")
        
        config = {"configurable": {"thread_id": "interactive_chain_rag_session"}}
        
        while True:
            user_input = input("\nYou: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'ë‚˜ê°€ê¸°']:
                print("Chain RAG ì±„íŒ… ì„¸ì…˜ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”!")
                break
            
            if not user_input:
                continue
            
            try:
                print("ë‹µë³€:")
                for step in self.app.stream(
                    {"messages": [{"role": "user", "content": user_input}]},
                    stream_mode="values",
                    config=config,
                ):
                    if step["messages"][-1].type == "ai" and not step["messages"][-1].tool_calls:
                        print(f"Bot: {step['messages'][-1].content}")
                        
            except Exception as e:
                print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                print("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    
    def interactive_agent_chat(self):
        """
        Start an interactive Agent-based RAG chat session.
        Agent decides when to search documents.
        """
        print("\n=== ì¸í„°ë™í‹°ë¸Œ Agent RAG ì±„íŒ… ===")
        print("Agentê°€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•´ì„œ ê²€ìƒ‰ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.")
        print("ì¼ë°˜ì ì¸ ì§ˆë¬¸ì€ ì§ì ‘ ë‹µë³€í•˜ê³ , ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸ì€ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        print("Agentì˜ ì¶”ë¡  ê³¼ì •ê³¼ ë„êµ¬ ì‚¬ìš©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("'quit'ì„ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
        
        config = {"configurable": {"thread_id": "interactive_agent_session"}}
        first_message = True
        
        while True:
            user_input = input("\nYou: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'ë‚˜ê°€ê¸°']:
                print("Agent RAG ì±„íŒ… ì„¸ì…˜ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”!")
                break
            
            if not user_input:
                continue
            
            try:
                print("\n" + "="*50)
                print("ğŸ¤– Agent ì¶”ë¡  ê³¼ì •:")
                print("-"*30)
                
                # Include system message for first interaction
                if first_message:
                    messages = [
                        {"role": "system", "content": self.agent_system_message},
                        {"role": "user", "content": user_input}
                    ]
                    first_message = False
                else:
                    messages = [{"role": "user", "content": user_input}]
                
                # Track agent steps
                step_count = 0
                tool_call_count = 0
                final_response = None
                
                # Stream agent response with detailed steps
                for chunk in self.agent_app.stream(
                    {"messages": messages},
                    config=config,
                    stream_mode="updates"
                ):
                    step_count += 1
                    
                    # Process each node update
                    for node_name, node_data in chunk.items():
                        if node_name == "__start__":
                            continue
                            
                        print(f"ğŸ“ Step {step_count}: {node_name}")
                        
                        if "messages" in node_data:
                            last_message = node_data["messages"][-1]
                            
                            # Check if this is a tool call
                            if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                                tool_call_count += 1
                                tool_name = last_message.tool_calls[0]['name']
                                tool_args = last_message.tool_calls[0]['args']
                                print(f"  ğŸ”§ ë„êµ¬ í˜¸ì¶œ #{tool_call_count}: {tool_name}")
                                print(f"     ğŸ“‹ ê²€ìƒ‰ì–´: {tool_args.get('query', 'N/A')}")
                                
                            # Check if this is a tool result
                            elif hasattr(last_message, 'type') and last_message.type == "tool":
                                print("  ğŸ“„ ë„êµ¬ ê²°ê³¼: ê²€ìƒ‰ ì™„ë£Œ")
                                doc_count = len(last_message.content.split('Source:')) - 1
                                print(f"     ğŸ“Š ë¬¸ì„œ ë°œê²¬: {doc_count}ê°œ")
                                
                            # Check if this is final AI response
                            elif hasattr(last_message, 'content') and last_message.type == "ai" and not (hasattr(last_message, 'tool_calls') and last_message.tool_calls):
                                print("  ğŸ’­ ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
                                final_response = last_message
                                
                        print()
                
                # Summary
                print("-"*30)
                print("ğŸ“Š Agent í™œë™ ìš”ì•½:")
                print(f"   â€¢ ì´ ë‹¨ê³„: {step_count}")
                print(f"   â€¢ ë„êµ¬ í˜¸ì¶œ íšŸìˆ˜: {tool_call_count}")
                print(f"   â€¢ ê²€ìƒ‰ ì—¬ë¶€: {'ê²€ìƒ‰í•¨' if tool_call_count > 0 else 'ê²€ìƒ‰ ì•ˆí•¨'}")
                print("-"*30)
                
                # Print final response
                if final_response and hasattr(final_response, 'content'):
                    print(f"ğŸ¯ ìµœì¢… ë‹µë³€:\n{final_response.content}")
                else:
                    print("âŒ ì‘ë‹µ ì—†ìŒ")
                
                print("="*50)
                        
            except Exception as e:
                print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                print("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")


def main():
    """
    Main function to demonstrate conversational RAG functionality.
    """
    print("ğŸ¤– Conversational RAG (ê²€ìƒ‰ ì¦ê°• ìƒì„±) ë°ëª¨")
    print("=" * 50)
    
    # Initialize RAG system
    rag_system = ConversationalRAG()
    
    # Get vector store choice
    print("\nì‚¬ìš©í•  ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. InMemory (ë©”ëª¨ë¦¬ ë‚´ ì €ì¥)")
    print("2. Chroma (ë¡œì»¬ ë””ìŠ¤í¬ ì €ì¥)")  
    print("3. FAISS (ê³ ì„±ëŠ¥ ê²€ìƒ‰)")
    
    vector_choice = input("ì„ íƒ (1/2/3): ").strip()
    if vector_choice not in ["1", "2", "3"]:
        print("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’(1)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        vector_choice = "1"
    
    # Setup system
    rag_system.setup_vector_store(vector_choice)
    rag_system.load_documents()
    
    # Get RAG type choice
    print("\nì‚¬ìš©í•  RAG ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. Chain RAG (ì²´ì¸ ë°©ì‹ - ëª¨ë“  ì§ˆë¬¸ì— ê²€ìƒ‰ ìˆ˜í–‰)")
    print("2. Agent RAG (ì—ì´ì „íŠ¸ ë°©ì‹ - ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•˜ì—¬ ê²€ìƒ‰)")
    
    rag_choice = input("ì„ íƒ (1/2): ").strip()
    
    if rag_choice == "1":
        # Chain-based RAG
        rag_system.setup_workflow()
        
        # Run tests
        rag_system.run_basic_test()
        rag_system.run_persistent_test()
        
        # Ask for interactive session
        print("\n" + "=" * 50)
        user_choice = input("\nì¸í„°ë™í‹°ë¸Œ Chain RAG ì±„íŒ…ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
        
        if user_choice in ['y', 'yes', 'ã…‡', 'ë„¤']:
            rag_system.interactive_chat()
        else:
            print("ë°ëª¨ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
    
    elif rag_choice == "2":
        # Agent-based RAG
        rag_system.setup_workflow()  # Also setup chain workflow for comparison
        rag_system.setup_agent_workflow()
        
        # Run tests
        rag_system.run_basic_test()  # Chain test for comparison
        rag_system.run_agent_test()  # Agent test
        
        # Ask for interactive session
        print("\n" + "=" * 50)
        user_choice = input("\nì¸í„°ë™í‹°ë¸Œ Agent RAG ì±„íŒ…ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
        
        if user_choice in ['y', 'yes', 'ã…‡', 'ë„¤']:
            rag_system.interactive_agent_chat()
        else:
            print("ë°ëª¨ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
    
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ë°ëª¨ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()