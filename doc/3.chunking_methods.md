# 5ê°€ì§€ Chunking ë°©ë²• ì™„ì „ ì •ë¦¬

## ğŸ“š ëª©ì°¨

- [ê°œìš”](#-ê°œìš”)
- [Chunking ë°©ë²• ë¹„êµí‘œ](#-chunking-ë°©ë²•-ë¹„êµí‘œ)
- [Level 1: Character Split](#-level-1-character-split)
- [Level 2: Recursive Character Split](#-level-2-recursive-character-split)
- [Level 3: Document Specific Splitting](#-level-3-document-specific-splitting)
- [Level 4: Semantic Splitting (With Embeddings)](#-level-4-semantic-splitting-with-embeddings)
- [Level 5: Agentic Splitting](#-level-5-agentic-splitting)
- [ì„ íƒ ê°€ì´ë“œ](#-ì„ íƒ-ê°€ì´ë“œ)
  - [ìƒí™©ë³„ ê¶Œì¥ì‚¬í•­](#-ìƒí™©ë³„-ê¶Œì¥ì‚¬í•­)
- [ì‹¤ì „ íŒ](#-ì‹¤ì „-íŒ)
  - [1. í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•](#1-í•˜ì´ë¸Œë¦¬ë“œ-ì ‘ê·¼ë²•)
  - [2. ì„±ëŠ¥ ìµœì í™”](#2-ì„±ëŠ¥-ìµœì í™”)
  - [3. ë¹„ìš© ê´€ë¦¬](#3-ë¹„ìš©-ê´€ë¦¬)
- [ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬](#-ì„±ëŠ¥-ë²¤ì¹˜ë§ˆí¬)
- [ê²°ë¡ ](#-ê²°ë¡ )

---

## ğŸ“‹ ê°œìš”

RAG ì‹œìŠ¤í…œì—ì„œ ë¬¸ì„œ ë¶„í• (Chunking)ì€ ì„±ëŠ¥ì„ ì¢Œìš°í•˜ëŠ” í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤. ë‹¨ìˆœí•œ ë¬¸ì ë¶„í• ë¶€í„° AI ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•˜ëŠ” ì§€ëŠ¥ì  ë¶„í• ê¹Œì§€, 5ê°€ì§€ ë ˆë²¨ë¡œ ë‚˜ëˆ„ì–´ ì„¤ëª…í•œë‹¤.

## ğŸ”„ Chunking ë°©ë²• ë¹„êµí‘œ

| ë ˆë²¨ | ë°©ë²• | ë³µì¡ë„ | í’ˆì§ˆ | ì†ë„ | ë¹„ìš© | ì ìš© ë¶„ì•¼ |
|------|------|---------|------|------|------|-----------|
| 1 | Character Split | â­ | â­ | â­â­â­â­â­ | ğŸ’° | í”„ë¡œí† íƒ€ì… |
| 2 | Recursive Character Split | â­â­ | â­â­â­ | â­â­â­â­ | ğŸ’° | ì¼ë°˜ í…ìŠ¤íŠ¸ |
| 3 | Document Specific Splitting | â­â­â­ | â­â­â­â­ | â­â­â­ | ğŸ’°ğŸ’° | êµ¬ì¡°í™”ëœ ë¬¸ì„œ |
| 4 | Semantic Splitting | â­â­â­â­ | â­â­â­â­â­ | â­â­ | ğŸ’°ğŸ’°ğŸ’° | ê³ í’ˆì§ˆ RAG |
| 5 | Agentic Splitting | â­â­â­â­â­ | â­â­â­â­â­ | â­ | ğŸ’°ğŸ’°ğŸ’°ğŸ’° | ì „ë¬¸ ë¶„ì•¼ |

---

## ğŸ“ Level 1: Character Split

### íŠ¹ì§•
- ê°€ì¥ ê¸°ë³¸ì ì¸ ë¬¸ì ë‹¨ìœ„ ë¶„í• 
- ê³ ì •ëœ ë¬¸ì ìˆ˜ë¡œ í…ìŠ¤íŠ¸ ë¶„í• 
- ë¬¸ë§¥ ê³ ë ¤ ì—†ìŒ

### ì½”ë“œ ì˜ˆì‹œ
```python
from langchain.text_splitter import CharacterTextSplitter

def character_split(text):
    """ê¸°ë³¸ ë¬¸ì ë¶„í• """
    splitter = CharacterTextSplitter(
        chunk_size=1000,     # 1000ì ë‹¨ìœ„ë¡œ ë¶„í• 
        chunk_overlap=100,   # 100ì ì¤‘ë³µ
        separator="\n"       # ì¤„ë°”ê¿ˆ ê¸°ì¤€
    )
    chunks = splitter.split_text(text)
    return chunks

# ì‚¬ìš© ì˜ˆì‹œ
text = "ê¸´ í…ìŠ¤íŠ¸ ë‚´ìš©..."
chunks = character_split(text)
print(f"ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
```

### ì¥ì  âœ…
- êµ¬í˜„ì´ ë§¤ìš° ê°„ë‹¨
- ì²˜ë¦¬ ì†ë„ê°€ ë¹ ë¦„
- ë¹„ìš©ì´ ì €ë ´

### ë‹¨ì  âŒ
- ë¬¸ë§¥ ë¬´ì‹œë¡œ ì˜ë¯¸ê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ
- ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ë¶„í•  ê°€ëŠ¥
- í’ˆì§ˆì´ ë‚®ìŒ

### ì ìš© ë¶„ì•¼
- ë¹ ë¥¸ í”„ë¡œí† íƒ€ì… ê°œë°œ
- ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
- ë‹¨ìˆœí•œ ì •ë³´ ê²€ìƒ‰

---

## ğŸ”„ Level 2: Recursive Character Split

### íŠ¹ì§•
- ê³„ì¸µì  êµ¬ë¶„ì ì‚¬ìš©
- ë¬¸ë‹¨ â†’ ë¬¸ì¥ â†’ ë‹¨ì–´ ìˆœìœ¼ë¡œ ë¶„í• 
- ë¬¸ë§¥ ë³´ì¡´ ê°œì„ 

### ì½”ë“œ ì˜ˆì‹œ
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

def recursive_character_split(text):
    """ì¬ê·€ì  ë¬¸ì ë¶„í• """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", " ", ""]  # ìš°ì„ ìˆœìœ„ êµ¬ë¶„ì
    )
    chunks = splitter.split_text(text)
    return chunks

# í–¥ìƒëœ ì„¤ì •
def advanced_recursive_split(text):
    """ê³ ê¸‰ ì¬ê·€ì  ë¶„í• """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=300,
        separators=[
            "\n\n",    # ë¬¸ë‹¨ êµ¬ë¶„
            "\n",      # ì¤„ êµ¬ë¶„
            ". ",      # ë¬¸ì¥ êµ¬ë¶„
            "! ",      # ê°íƒ„ë¬¸ êµ¬ë¶„
            "? ",      # ì˜ë¬¸ë¬¸ êµ¬ë¶„
            " ",       # ë‹¨ì–´ êµ¬ë¶„
            ""         # ë¬¸ì êµ¬ë¶„
        ]
    )
    return splitter.split_text(text)
```

### ì¥ì  âœ…
- ë¬¸ë§¥ ë³´ì¡´ì´ ê°œì„ ë¨
- ìœ ì—°í•œ êµ¬ë¶„ì ì„¤ì •
- ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì í•©

### ë‹¨ì  âŒ
- ì—¬ì „íˆ ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  ì•„ë‹˜
- ë¬¸ì„œ êµ¬ì¡° ê³ ë ¤ ë¶€ì¡±
- ë³µì¡í•œ í˜•ì‹ì— í•œê³„

### ì ìš© ë¶„ì•¼
- ì¼ë°˜ì ì¸ RAG ì‹œìŠ¤í…œ
- ë¸”ë¡œê·¸, ë‰´ìŠ¤ ê¸°ì‚¬
- ì±…, ë…¼ë¬¸ ë“± ì¼ë°˜ ë¬¸ì„œ

---

## ğŸ“Š Level 3: Document Specific Splitting

### íŠ¹ì§•
- ë¬¸ì„œ í˜•ì‹ë³„ íŠ¹í™” ë¶„í• 
- êµ¬ì¡° ì •ë³´ í™œìš©
- ë©”íƒ€ë°ì´í„° ë³´ì¡´

### ì½”ë“œ ì˜ˆì‹œ
```python
from langchain.text_splitter import (
    PythonCodeTextSplitter,
    MarkdownTextSplitter,
    HTMLHeaderTextSplitter
)

def document_specific_split(content, doc_type):
    """ë¬¸ì„œ í˜•ì‹ë³„ ë¶„í• """
    
    if doc_type == "python":
        splitter = PythonCodeTextSplitter(
            chunk_size=1000,
            chunk_overlap=100
        )
    elif doc_type == "markdown":
        splitter = MarkdownTextSplitter(
            chunk_size=1000,
            chunk_overlap=100
        )
    elif doc_type == "html":
        splitter = HTMLHeaderTextSplitter(
            headers_to_split_on=[
                ("h1", "Header 1"),
                ("h2", "Header 2"),
                ("h3", "Header 3")
            ]
        )
    else:
        # ê¸°ë³¸ ì¬ê·€ì  ë¶„í• 
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100
        )
    
    return splitter.split_text(content)

# Markdown íŠ¹í™” ì˜ˆì‹œ
def markdown_split_with_metadata(md_content):
    """ë©”íƒ€ë°ì´í„° ë³´ì¡´í•˜ëŠ” ë§ˆí¬ë‹¤ìš´ ë¶„í• """
    headers_to_split_on = [
        ("# ", "Header 1"),
        ("## ", "Header 2"),
        ("### ", "Header 3"),
    ]
    
    splitter = MarkdownTextSplitter.from_tiktoken_encoder(
        chunk_size=1000,
        chunk_overlap=100,
        headers_to_split_on=headers_to_split_on
    )
    
    return splitter.split_text(md_content)
```

### ì¥ì  âœ…
- ë¬¸ì„œ êµ¬ì¡° ì •ë³´ í™œìš©
- ë©”íƒ€ë°ì´í„° ë³´ì¡´
- í˜•ì‹ë³„ ìµœì í™”

### ë‹¨ì  âŒ
- ë¬¸ì„œ íƒ€ì…ë³„ ì„¤ì • í•„ìš”
- ë³µì¡í•œ êµ¬í˜„
- ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  ì•„ë‹˜

### ì ìš© ë¶„ì•¼
- ì½”ë“œ ë¬¸ì„œí™”
- ê¸°ìˆ  ë¬¸ì„œ
- êµ¬ì¡°í™”ëœ ì›¹ í˜ì´ì§€

---

## ğŸ§  Level 4: Semantic Splitting (With Embeddings)

### íŠ¹ì§•
- ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ ë¶„í• 
- ì˜ë¯¸ì  ìœ ì‚¬ì„± ê³ ë ¤
- ë¬¸ë§¥ ë³´ì¡´ ê·¹ëŒ€í™”

### ì½”ë“œ ì˜ˆì‹œ
```python
from langchain.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings
import numpy as np

def semantic_split(text):
    """ì˜ë¯¸ì  ë¶„í• """
    embeddings = OpenAIEmbeddings()
    
    splitter = SemanticChunker(
        embeddings=embeddings,
        breakpoint_threshold_type="percentile",  # ë¶„í•  ì„ê³„ê°’ ë°©ì‹
        breakpoint_threshold_amount=95           # 95% ì„ê³„ê°’
    )
    
    chunks = splitter.split_text(text)
    return chunks

def custom_semantic_split(text, threshold=0.7):
    """ì»¤ìŠ¤í…€ ì˜ë¯¸ì  ë¶„í• """
    embeddings = OpenAIEmbeddings()
    
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
    sentences = text.split('. ')
    
    # ê° ë¬¸ì¥ ì„ë² ë”© ìƒì„±
    sentence_embeddings = embeddings.embed_documents(sentences)
    
    chunks = []
    current_chunk = [sentences[0]]
    
    for i in range(1, len(sentences)):
        # í˜„ì¬ ì²­í¬ì™€ ë‹¤ìŒ ë¬¸ì¥ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        current_embedding = np.mean([sentence_embeddings[j] for j in range(len(current_chunk))], axis=0)
        next_embedding = sentence_embeddings[i]
        
        similarity = np.dot(current_embedding, next_embedding) / (
            np.linalg.norm(current_embedding) * np.linalg.norm(next_embedding)
        )
        
        if similarity > threshold:
            current_chunk.append(sentences[i])
        else:
            # ìƒˆë¡œìš´ ì²­í¬ ì‹œì‘
            chunks.append('. '.join(current_chunk))
            current_chunk = [sentences[i]]
    
    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    chunks.append('. '.join(current_chunk))
    
    return chunks
```

### ì¥ì  âœ…
- ì˜ë¯¸ ê¸°ë°˜ ë¶„í• 
- ë¬¸ë§¥ ë³´ì¡´ ìš°ìˆ˜
- ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ

### ë‹¨ì  âŒ
- ë†’ì€ ê³„ì‚° ë¹„ìš©
- ì„ë² ë”© ìƒì„± ì‹œê°„ ì†Œìš”
- ë³µì¡í•œ êµ¬í˜„

### ì ìš© ë¶„ì•¼
- ê³ í’ˆì§ˆ RAG ì‹œìŠ¤í…œ
- ì „ë¬¸ ì§€ì‹ ê²€ìƒ‰
- ì •í™•ì„±ì´ ì¤‘ìš”í•œ ë¶„ì•¼

---

## ğŸ¤– Level 5: Agentic Splitting

### íŠ¹ì§•
- AI ì—ì´ì „íŠ¸ê°€ ë¶„í•  ê²°ì •
- ë¬¸ì„œ ë‚´ìš© ì´í•´ í›„ ë¶„í• 
- ìµœê³  ìˆ˜ì¤€ì˜ ì§€ëŠ¥ì  ë¶„í• 

### ì½”ë“œ ì˜ˆì‹œ
```python
from langchain.agents import initialize_agent
from langchain.tools import Tool
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate

def agentic_split(text):
    """ì—ì´ì „íŠ¸ ê¸°ë°˜ ë¶„í• """
    
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    
    # ë¶„í•  í”„ë¡¬í”„íŠ¸
    splitting_prompt = PromptTemplate(
        input_variables=["text"],
        template="""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ì¼ê´€ëœ ì²­í¬ë¡œ ë¶„í• í•˜ì„¸ìš”.
        ê° ì²­í¬ëŠ” ì™„ì „í•œ ê°œë…ì´ë‚˜ ì•„ì´ë””ì–´ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
        
        ê·œì¹™:
        1. ê° ì²­í¬ëŠ” 500-1500 ë‹¨ì–´ ì‚¬ì´
        2. ë¬¸ë§¥ìƒ ì—°ê´€ëœ ë‚´ìš©ì€ ê°™ì€ ì²­í¬ì— í¬í•¨
        3. ìƒˆë¡œìš´ ì£¼ì œë‚˜ ì„¹ì…˜ì€ ìƒˆë¡œìš´ ì²­í¬ë¡œ ë¶„í• 
        4. ê° ì²­í¬ì— ê°„ë‹¨í•œ ì œëª© ë¶€ì—¬
        
        í…ìŠ¤íŠ¸:
        {text}
        
        ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
        [CHUNK_1]
        ì œëª©: 
        ë‚´ìš©: 
        [CHUNK_2]
        ì œëª©: 
        ë‚´ìš©: 
        ...
        """
    )
    
    # ì—ì´ì „íŠ¸ ì‹¤í–‰
    chain = splitting_prompt | llm
    result = chain.invoke({"text": text})
    
    # ê²°ê³¼ íŒŒì‹±
    chunks = parse_agentic_result(result.content)
    return chunks

def parse_agentic_result(result):
    """ì—ì´ì „íŠ¸ ê²°ê³¼ íŒŒì‹±"""
    chunks = []
    current_chunk = {}
    
    lines = result.split('\n')
    for line in lines:
        line = line.strip()
        if line.startswith('[CHUNK_'):
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = {}
        elif line.startswith('ì œëª©:'):
            current_chunk['title'] = line.replace('ì œëª©:', '').strip()
        elif line.startswith('ë‚´ìš©:'):
            current_chunk['content'] = line.replace('ë‚´ìš©:', '').strip()
        elif line and 'content' in current_chunk:
            current_chunk['content'] += ' ' + line
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks

def advanced_agentic_split(text, domain="general"):
    """ë„ë©”ì¸ë³„ ì—ì´ì „íŠ¸ ë¶„í• """
    
    domain_prompts = {
        "technical": "ê¸°ìˆ  ë¬¸ì„œì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ê°œë…ë³„ë¡œ ë¶„í• ",
        "legal": "ë²•ë¥  ë¬¸ì„œì˜ ì¡°í•­ê³¼ ê·œì •ì„ ê³ ë ¤í•˜ì—¬ ë¶„í• ",
        "medical": "ì˜ë£Œ ì •ë³´ì˜ ì—°ê´€ì„±ì„ ê³ ë ¤í•˜ì—¬ ë¶„í• ",
        "general": "ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ë¶„í•  ê·œì¹™ ì ìš©"
    }
    
    specific_instruction = domain_prompts.get(domain, domain_prompts["general"])
    
    # ë„ë©”ì¸ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
    # ... (êµ¬í˜„ ìƒëµ)
    
    return chunks
```

### ì¥ì  âœ…
- ìµœê³  ìˆ˜ì¤€ì˜ ì§€ëŠ¥ì  ë¶„í• 
- ë„ë©”ì¸ë³„ íŠ¹í™” ê°€ëŠ¥
- ë¬¸ì„œ ë‚´ìš© ì™„ì „ ì´í•´

### ë‹¨ì  âŒ
- ë§¤ìš° ë†’ì€ ë¹„ìš©
- ì²˜ë¦¬ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼
- ë³µì¡í•œ êµ¬í˜„

### ì ìš© ë¶„ì•¼
- ì „ë¬¸ ì§€ì‹ ì‹œìŠ¤í…œ
- ê³ ë¶€ê°€ê°€ì¹˜ ì»¨í…ì¸ 
- ì •ë°€ ë¶„ì„ì´ í•„ìš”í•œ ë¶„ì•¼

---

## ğŸ¯ ì„ íƒ ê°€ì´ë“œ

### ğŸ“Š ìƒí™©ë³„ ê¶Œì¥ì‚¬í•­

#### ë¹ ë¥¸ í”„ë¡œí† íƒ€ì… ê°œë°œ
```python
# Level 1: Character Split
splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
```

#### ì¼ë°˜ì ì¸ RAG ì‹œìŠ¤í…œ
```python
# Level 2: Recursive Character Split
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000, 
    chunk_overlap=200,
    separators=["\n\n", "\n", ". ", " ", ""]
)
```

#### êµ¬ì¡°í™”ëœ ë¬¸ì„œ ì²˜ë¦¬
```python
# Level 3: Document Specific Splitting
splitter = MarkdownTextSplitter(chunk_size=1000, chunk_overlap=100)
```

#### ê³ í’ˆì§ˆ ê²€ìƒ‰ ì‹œìŠ¤í…œ
```python
# Level 4: Semantic Splitting
splitter = SemanticChunker(embeddings=OpenAIEmbeddings())
```

#### ì „ë¬¸ ë¶„ì•¼ ì‹œìŠ¤í…œ
```python
# Level 5: Agentic Splitting
chunks = agentic_split(text)
```

## ğŸ”§ ì‹¤ì „ íŒ

### 1. í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•
```python
def hybrid_chunking(text, doc_type="general"):
    """ìƒí™©ì— ë§ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ë¶„í• """
    
    # 1ë‹¨ê³„: ë¬¸ì„œ í˜•ì‹ë³„ ë¶„í• 
    if doc_type == "markdown":
        primary_chunks = MarkdownTextSplitter().split_text(text)
    else:
        primary_chunks = RecursiveCharacterTextSplitter().split_text(text)
    
    # 2ë‹¨ê³„: ê¸´ ì²­í¬ëŠ” ì˜ë¯¸ì  ë¶„í• 
    final_chunks = []
    for chunk in primary_chunks:
        if len(chunk) > 2000:  # ë„ˆë¬´ ê¸´ ì²­í¬
            semantic_chunks = semantic_split(chunk)
            final_chunks.extend(semantic_chunks)
        else:
            final_chunks.append(chunk)
    
    return final_chunks
```

### 2. ì„±ëŠ¥ ìµœì í™”
```python
def optimized_chunking(text, quality_level="medium"):
    """ì„±ëŠ¥ê³¼ í’ˆì§ˆì˜ ê· í˜•"""
    
    if quality_level == "fast":
        return CharacterTextSplitter().split_text(text)
    elif quality_level == "medium":
        return RecursiveCharacterTextSplitter().split_text(text)
    elif quality_level == "high":
        return semantic_split(text)
    else:  # premium
        return agentic_split(text)
```

### 3. ë¹„ìš© ê´€ë¦¬
```python
def cost_effective_chunking(text, budget="low"):
    """ë¹„ìš©ì„ ê³ ë ¤í•œ ë¶„í• """
    
    budget_strategies = {
        "low": CharacterTextSplitter(),
        "medium": RecursiveCharacterTextSplitter(),
        "high": SemanticChunker(embeddings=OpenAIEmbeddings())
    }
    
    splitter = budget_strategies.get(budget, budget_strategies["low"])
    return splitter.split_text(text)
```

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ì²˜ë¦¬ ì‹œê°„ ë¹„êµ (1MB í…ìŠ¤íŠ¸ ê¸°ì¤€)
- Level 1: ~0.1ì´ˆ
- Level 2: ~0.5ì´ˆ
- Level 3: ~1.0ì´ˆ
- Level 4: ~30ì´ˆ
- Level 5: ~300ì´ˆ

### ë¹„ìš© ë¹„êµ (1MB í…ìŠ¤íŠ¸ ê¸°ì¤€)
- Level 1: ë¬´ë£Œ
- Level 2: ë¬´ë£Œ
- Level 3: ë¬´ë£Œ
- Level 4: ~$0.10
- Level 5: ~$5.00

## ğŸ¬ ê²°ë¡ 

Chunking ë°©ë²• ì„ íƒì€ **ìš©ë„, ì˜ˆì‚°, ì‹œê°„**ì„ ëª¨ë‘ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤:

1. **í”„ë¡œí† íƒ€ì…**: Level 1-2 ì‚¬ìš©
2. **í”„ë¡œë•ì…˜**: Level 2-3 ê¶Œì¥
3. **ê³ í’ˆì§ˆ**: Level 4-5 ê³ ë ¤
4. **í•˜ì´ë¸Œë¦¬ë“œ**: ìƒí™©ì— ë§ê²Œ ì¡°í•©

ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ **ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ì— ë§ëŠ” ë°©ë²•ì„ ì„ íƒ**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í•­ìƒ ì‘ì€ ê·œëª¨ë¡œ í…ŒìŠ¤íŠ¸í•œ í›„ í™•ì¥í•´ì•¼í•©ë‹ˆë‹¤.