# RAG Generation Methods

## Overview
RAG (Retrieval-Augmented Generation)ì—ì„œ Generation ë‹¨ê³„ëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•µì‹¬ ê³¼ì •ì…ë‹ˆë‹¤. ì£¼ìš” 3ê°€ì§€ ë°©ì‹ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.

## ğŸ”— RAG Pipeline Structure

```
Query â†’ Retrieval â†’ Context Formation â†’ Generation â†’ Response
```

## ğŸ“Š Generation Methods Comparison

### 1. **Manual Retrieval + Custom Prompt** (ê¸°ë³¸ ë°©ì‹)
```python
# Step 1: Manual retrieval
retriever = vectorstore.as_retriever()
docs = retriever.get_relevant_documents("What is Task Decomposition?")

# Step 2: Custom prompt
template = """Answer the question based only on the following context:
    {context}

    Question: {question}
    """

prompt = ChatPromptTemplate.from_template(template)
chain = prompt | llm
result = chain.invoke({"context": docs, "question": question})
```

**íŠ¹ì§•:**
- âœ… ì™„ì „í•œ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥
- âœ… ê°„ë‹¨í•˜ê³  ì§ê´€ì 
- âŒ ë§¤ë²ˆ ìˆ˜ë™ ê²€ìƒ‰ í•„ìš”
- âŒ í”„ë¡¬í”„íŠ¸ ìµœì í™” ë¶€ì¡±

### 2. **Manual Retrieval + Hub Prompt** (ê°œì„ ëœ ê¸°ë³¸ ë°©ì‹)
```python
# Step 1: Manual retrieval  
retriever = vectorstore.as_retriever()
docs = retriever.get_relevant_documents("What is Task Decomposition?")

# Step 2: Hub prompt
prompt_hub_rag = hub.pull("rlm/rag-prompt")
chain = prompt_hub_rag | llm
result = chain.invoke({"context": docs, "question": question})
```

**íŠ¹ì§•:**
- âœ… ê²€ì¦ëœ ìµœì í™” í”„ë¡¬í”„íŠ¸
- âœ… ì»¤ë®¤ë‹ˆí‹° ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤
- âœ… ì¼ê´€ëœ í’ˆì§ˆ ë³´ì¥
- âŒ ì—¬ì „íˆ ìˆ˜ë™ ê²€ìƒ‰ í•„ìš”

### 3. **Automated RAG Chain** (ì™„ì „ ìë™í™”)
```python
# Can use either custom or hub prompt
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | hub.pull("rlm/rag-prompt")  # ğŸ”„ Or custom_prompt
    | llm
    | StrOutputParser()
)
result = rag_chain.invoke("What is Task Decomposition?")
```

**íŠ¹ì§•:**
- âœ… ì™„ì „ ìë™í™”ëœ íŒŒì´í”„ë¼ì¸
- âœ… ìë™ ë¬¸ì„œ ê²€ìƒ‰ ë° í¬ë§·íŒ…
- âœ… Hub ë˜ëŠ” ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥
- âœ… ë†’ì€ ì¬ì‚¬ìš©ì„±
- âš ï¸ êµ¬ì¡° ë³€ê²½ ì‹œ ë³µì¡ì„± ì¦ê°€

## ğŸ” Key Components

### Pipe Operator (`|`)
LangChainì˜ ì²´ì¸ ì—°ê²° ì—°ì‚°ìë¡œ ë°ì´í„° íë¦„ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬:

```python
input â†’ component1 | component2 | component3 â†’ output
```

### format_docs Function
```python
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
```
Document ê°ì²´ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ LLMì´ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•¨.

### RunnablePassthrough
```python
{"context": retriever | format_docs, "question": RunnablePassthrough()}
```
ì…ë ¥ ë°ì´í„°ë¥¼ ë³€ê²½ ì—†ì´ ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ëŠ” ì»´í¬ë„ŒíŠ¸.

## ğŸ“ˆ Performance Comparison

| Method | Automation | Quality | Flexibility | Production Ready |
|--------|------------|---------|-------------|------------------|
| Manual Retrieval + Custom Prompt | âŒ Low | âš ï¸ Variable | âœ… High | âŒ No |
| Manual Retrieval + Hub Prompt | âš ï¸ Medium | âœ… High | âš ï¸ Medium | âš ï¸ Partial |
| Automated RAG Chain | âœ… High | âœ… High | âš ï¸ Medium | âœ… Yes |

## ğŸ¯ Best Practices

### 1. **Development Phase**
- ì‹œì‘: Manual Retrieval + Custom Promptìœ¼ë¡œ ê°œë… ì´í•´
- ê°œì„ : Manual Retrieval + Hub Promptë¡œ í’ˆì§ˆ í–¥ìƒ
- ì™„ì„±: Automated RAG Chainìœ¼ë¡œ ìë™í™”

### 2. **Production Use**
```python
# Recommended approach
def create_rag_chain(web_url: str, model_name: str = "gpt-3.5-turbo"):
    # Load documents
    loader = WebBaseLoader(web_paths=(web_url,))
    docs = loader.load()
    
    # Split and vectorize
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())
    retriever = vectorstore.as_retriever()
    
    # Build RAG chain
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI(model_name=model_name, temperature=0)
    
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain
```

### 3. **Usage Example**
```python
# One-time setup
rag_chain = create_rag_chain("https://example.com/docs")

# Multiple queries
questions = [
    "What is the main topic?",
    "How does this work?",
    "What are the benefits?"
]

for question in questions:
    answer = rag_chain.invoke(question)
    print(f"Q: {question}")
    print(f"A: {answer}\n")
```

## ğŸ”§ Advanced Tips

### Custom Prompt Templates
```python
# Create custom prompt while keeping automation
custom_prompt = ChatPromptTemplate.from_template(
    """You are an expert assistant. Answer based on the context.
    Context: {context}
    Question: {question}
    Answer in Korean:"""
)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | custom_prompt  # Use custom instead of hub prompt
    | llm
    | StrOutputParser()
)
```

### Multiple Retrievers
```python
# Combine multiple sources
web_retriever = web_vectorstore.as_retriever()
doc_retriever = doc_vectorstore.as_retriever()

def multi_format_docs(question):
    web_docs = web_retriever.get_relevant_documents(question)
    doc_docs = doc_retriever.get_relevant_documents(question)
    return format_docs(web_docs + doc_docs)
```

## ğŸ“ Summary

RAG Generation ë°©ì‹ì€ **ìë™í™” ìˆ˜ì¤€**ê³¼ **í’ˆì§ˆ**ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤:

1. **í•™ìŠµ ë‹¨ê³„**: Manual Retrieval + Custom Promptìœ¼ë¡œ ê°œë… ì´í•´
2. **ê°œë°œ ë‹¨ê³„**: Manual Retrieval + Hub Promptë¡œ í’ˆì§ˆ í™•ë³´  
3. **ìš´ì˜ ë‹¨ê³„**: Automated RAG Chainìœ¼ë¡œ ì™„ì „ ìë™í™”

**Automated RAG Chain**ì´ ê°€ì¥ ì‹¤ìš©ì ì´ë©°, í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ê¶Œì¥ë˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. 