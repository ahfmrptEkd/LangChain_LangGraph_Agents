# RAG Generation Methods

## Overview
RAG (Retrieval-Augmented Generation)에서 Generation 단계는 검색된 문서를 기반으로 최종 답변을 생성하는 핵심 과정입니다. 주요 3가지 방식을 비교 분석합니다.

## 🔗 RAG Pipeline Structure

```
Query → Retrieval → Context Formation → Generation → Response
```

## 📊 Generation Methods Comparison

### 1. **Manual Retrieval + Custom Prompt** (기본 방식)
```python
# Step 1: Manual retrieval
retriever = vectorstore.as_retriever()
docs = retriever.get_relevant_documents("What is Task Decomposition?")

# Step 2: Custom prompt
template = """Answer the question based only on the following context:
    {context}

    Question: {question}
    """

prompt = ChatPromptTemplate.from_template(template)
chain = prompt | llm
result = chain.invoke({"context": docs, "question": question})
```

**특징:**
- ✅ 완전한 커스터마이징 가능
- ✅ 간단하고 직관적
- ❌ 매번 수동 검색 필요
- ❌ 프롬프트 최적화 부족

### 2. **Manual Retrieval + Hub Prompt** (개선된 기본 방식)
```python
# Step 1: Manual retrieval  
retriever = vectorstore.as_retriever()
docs = retriever.get_relevant_documents("What is Task Decomposition?")

# Step 2: Hub prompt
prompt_hub_rag = hub.pull("rlm/rag-prompt")
chain = prompt_hub_rag | llm
result = chain.invoke({"context": docs, "question": question})
```

**특징:**
- ✅ 검증된 최적화 프롬프트
- ✅ 커뮤니티 베스트 프랙티스
- ✅ 일관된 품질 보장
- ❌ 여전히 수동 검색 필요

### 3. **Automated RAG Chain** (완전 자동화)
```python
# Can use either custom or hub prompt
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | hub.pull("rlm/rag-prompt")  # 🔄 Or custom_prompt
    | llm
    | StrOutputParser()
)
result = rag_chain.invoke("What is Task Decomposition?")
```

**특징:**
- ✅ 완전 자동화된 파이프라인
- ✅ 자동 문서 검색 및 포맷팅
- ✅ Hub 또는 커스텀 프롬프트 모두 사용 가능
- ✅ 높은 재사용성
- ⚠️ 구조 변경 시 복잡성 증가

## 🔍 Key Components

### Pipe Operator (`|`)
LangChain의 체인 연결 연산자로 데이터 흐름을 순차적으로 처리:

```python
input → component1 | component2 | component3 → output
```

### format_docs Function
```python
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
```
Document 객체들을 문자열로 변환하여 LLM이 처리할 수 있도록 함.

### RunnablePassthrough
```python
{"context": retriever | format_docs, "question": RunnablePassthrough()}
```
입력 데이터를 변경 없이 그대로 전달하는 컴포넌트.

## 📈 Performance Comparison

| Method | Automation | Quality | Flexibility | Production Ready |
|--------|------------|---------|-------------|------------------|
| Manual Retrieval + Custom Prompt | ❌ Low | ⚠️ Variable | ✅ High | ❌ No |
| Manual Retrieval + Hub Prompt | ⚠️ Medium | ✅ High | ⚠️ Medium | ⚠️ Partial |
| Automated RAG Chain | ✅ High | ✅ High | ⚠️ Medium | ✅ Yes |

## 🎯 Best Practices

### 1. **Development Phase**
- 시작: Manual Retrieval + Custom Prompt으로 개념 이해
- 개선: Manual Retrieval + Hub Prompt로 품질 향상
- 완성: Automated RAG Chain으로 자동화

### 2. **Production Use**
```python
# Recommended approach
def create_rag_chain(web_url: str, model_name: str = "gpt-3.5-turbo"):
    # Load documents
    loader = WebBaseLoader(web_paths=(web_url,))
    docs = loader.load()
    
    # Split and vectorize
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())
    retriever = vectorstore.as_retriever()
    
    # Build RAG chain
    prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI(model_name=model_name, temperature=0)
    
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain
```

### 3. **Usage Example**
```python
# One-time setup
rag_chain = create_rag_chain("https://example.com/docs")

# Multiple queries
questions = [
    "What is the main topic?",
    "How does this work?",
    "What are the benefits?"
]

for question in questions:
    answer = rag_chain.invoke(question)
    print(f"Q: {question}")
    print(f"A: {answer}\n")
```

## 🔧 Advanced Tips

### Custom Prompt Templates
```python
# Create custom prompt while keeping automation
custom_prompt = ChatPromptTemplate.from_template(
    """You are an expert assistant. Answer based on the context.
    Context: {context}
    Question: {question}
    Answer in Korean:"""
)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | custom_prompt  # Use custom instead of hub prompt
    | llm
    | StrOutputParser()
)
```

### Multiple Retrievers
```python
# Combine multiple sources
web_retriever = web_vectorstore.as_retriever()
doc_retriever = doc_vectorstore.as_retriever()

def multi_format_docs(question):
    web_docs = web_retriever.get_relevant_documents(question)
    doc_docs = doc_retriever.get_relevant_documents(question)
    return format_docs(web_docs + doc_docs)
```

## 📝 Summary

RAG Generation 방식은 **자동화 수준**과 **품질**의 균형을 맞추는 것이 핵심입니다:

1. **학습 단계**: Manual Retrieval + Custom Prompt으로 개념 이해
2. **개발 단계**: Manual Retrieval + Hub Prompt로 품질 확보  
3. **운영 단계**: Automated RAG Chain으로 완전 자동화

**Automated RAG Chain**이 가장 실용적이며, 프로덕션 환경에서 권장되는 방식입니다. 